To extend this project beyond just parsing amazon.com pages we would have to look at the ruleset class. Right now only one set of rules is defined in this class. But it would not be very hard to update it so we could choose a set of rules depending on a provided domain. In this way we could define rules for different domains and choose them as needed.

In these rulesets we can have any arbitrary keys for our hashes. This mean rulesets can easily be for things other than books. The only key required is a "numerical_weight" field. This just stores the weight as a number.

Currently the parsing process takes just O(n) + whatever time it takes to actually apply the rules. The real bottleneck on time is the bin packing algorithm. I implemented a simple algorithm that sorts the bins and books by descending weight and adds the books to the first bin that it fits in. This is not the fastest algorithm but it is still polynomial. It comes out to being roughly O(n * b(log(b))) where n is the number of books and b is the number of bins. A better way to implement this algorithm would be to store the amount of empty space each bin has in a 2-3 tree. We would then search through it and find the bin with the smallest amount of space that the book can fit in. This search process would only take log(b) and thus the whole thing would only take O(n log(b)). This would help us scale to millions of books while taking less time.
