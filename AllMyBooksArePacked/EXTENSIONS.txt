1. For domains beyond Amazon.com, you would have to write a separate grab method for each domain. For example, the Crawler.grab_book would be renamed Crawler.grab_book_from_amazon and you would have more methods based on domain and product. The Crawler.grab_page function could be expanded to return the Nokogiri HTML Object and the domain of the page (maybe using image (logo) recognition or you can scan for words in H1 elements against a list of domain names that you have methods for). This could then be passed to a helper method which would call the proper method based on the domain returned from the Crawler.grab_page method. 

2. For products, you will have to create grab method for each type of product. I looked through other Amazon products and even though they are close to having the same css selctors, it's not quite the same. Plus you might want to have different properties based on product. As with the domain, you would expand the Crawler.grab_page function to return the Nokogiri HTML Object, domain, and the product. And then expand the helper function to call the proper grab_<blank>_from_<blank> function on your Nokogiri HTML Object.

3. I used the Greedy algorithm to sort the books with a running time of n * log(n) (http://en.wikipedia.org/wiki/Partition_problem). Currently, there is no check to determine if any of the products shipping weight is larger than the desired split weight.